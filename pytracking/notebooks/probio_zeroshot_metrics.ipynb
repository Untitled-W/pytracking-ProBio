{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouTubeVOS/2019/probio_valid loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams['figure.figsize'] = [14, 8]\n",
    "import motmetrics as mm\n",
    "from motmetrics.distances import iou_matrix\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../..')\n",
    "from pytracking.utils.load_text import load_text\n",
    "from pytracking.evaluation import Tracker, get_dataset, trackerlist\n",
    "from pprint import pprint\n",
    "\n",
    "trackers = []\n",
    "# dataset = get_dataset('probio')\n",
    "# dataset = get_dataset('yt2019_sub')\n",
    "dataset = get_dataset('yt_pb_valid')\n",
    "\n",
    "def motMetricsEnhancedCalculator(gt, t):\n",
    "  # import required packages\n",
    "  \n",
    "  acc = mm.MOTAccumulator(auto_id=True)\n",
    "\n",
    "  # Max frame number maybe different for gt and t files\n",
    "  for frame in range(int(gt[:,0].max())):\n",
    "    frame += 1 # detection and frame numbers begin at 1\n",
    "\n",
    "    # select id, x, y, width, height for current frame\n",
    "    # required format for distance calculation is X, Y, Width, Height \\\n",
    "    # We already have this format\n",
    "    gt_dets = gt[gt[:,0]==frame,1:6] # select all detections in gt\n",
    "    t_dets = t[t[:,0]==frame,1:6] # select all detections in t\n",
    "\n",
    "    C = mm.distances.iou_matrix(gt_dets[:,1:], t_dets[:,1:], \\\n",
    "                                max_iou=0.5) # format: gt, t\n",
    "\n",
    "    # Call update once for per frame.\n",
    "    # format: gt object ids, t object ids, distance\n",
    "    acc.update(gt_dets[:,0].astype('int').tolist(), \\\n",
    "              t_dets[:,0].astype('int').tolist(), C)\n",
    "    \n",
    "  return acc\n",
    "\n",
    "def one_track(trk):\n",
    "    names = []\n",
    "    accs = []\n",
    "\n",
    "    for seq_id, seq in enumerate(dataset):\n",
    "        base_results_path = '{}/{}'.format(trk.results_dir, seq.name)\n",
    "        gt = []\n",
    "        pred = []\n",
    "        for (obj_id, obj_gt) in list(seq.ground_truth_rect.items()):\n",
    "            results_path = '{}_{}.txt'.format(base_results_path,obj_id)\n",
    "            if os.path.isfile(results_path):\n",
    "                pred_bb = torch.tensor(load_text(str(results_path), delimiter=('\\t', ','), dtype=np.float64))\n",
    "                obj_gt = torch.tensor(obj_gt,dtype=torch.float64)\n",
    "            else:\n",
    "                print('Result not found. {}'.format(results_path))\n",
    "                continue\n",
    "            for frame_id, b in enumerate(obj_gt):\n",
    "                if b[0] == -1: continue\n",
    "                gt.append([frame_id+1,obj_id,b[0],b[1],b[2],b[3],1.,-1.,-1.,-1.])\n",
    "            for frame_id, b in enumerate(pred_bb):\n",
    "                if b[0] == -1: continue\n",
    "                pred.append([frame_id+1,obj_id,b[0],b[1],b[2],b[3],1.,-1.,-1.,-1.])\n",
    "        # if seq.name in ['0043f083b5','0044fa5fba']:\n",
    "        #     slices = slice(2,6)\n",
    "        #     print(f'----------------{seq.name}-------------')\n",
    "        #     cur_id = 0\n",
    "        #     for i,j in zip(gt,pred):\n",
    "        #         if i[1] != cur_id:\n",
    "        #             print(f'----------------{i[1]}-------------')\n",
    "        #             cur_id = i[1]\n",
    "        #         a = [f'{int(ii)-int(jj):>3}' for ii,jj in zip(i[slices],j[slices])]\n",
    "        #         bi = [int(ii) for ii in i[slices]]\n",
    "        #         bj = [int(jj) for jj in j[slices]]\n",
    "        #         c = iou_matrix([i[slices]],[j[slices]])[0][0]\n",
    "        #         print(f'{c:>5.3f}',a,bi,bj)\n",
    "        if len(gt) == 0: continue\n",
    "        accs.append(motMetricsEnhancedCalculator(np.array(gt, dtype=np.float64), np.array(pred, dtype=np.float64)))\n",
    "        names.append(seq.name)\n",
    "\n",
    "\n",
    "    mh = mm.metrics.create()\n",
    "    summary = mh.compute_many(accs, metrics=['mota', 'motp', 'num_frames', 'num_objects' , 'idf1', 'idp', 'idr', \\\n",
    "                                        'recall', 'precision',  \\\n",
    "                                        'mostly_tracked', 'partially_tracked', \\\n",
    "                                        'mostly_lost', 'num_false_positives', \\\n",
    "                                        'num_misses', 'num_switches', \\\n",
    "                                        'num_fragmentations'\n",
    "                                    ], \\\n",
    "                        generate_overall=True,\n",
    "                        names=names)\n",
    "\n",
    "    # strsummary = mm.io.render_summary(\n",
    "    #     summary,\n",
    "    #     #formatters={'mota' : '{:.2%}'.format},\n",
    "    #     namemap={'num_objects': 'GT', 'mota': 'MOTA', 'motp' : 'MOTP', \\\n",
    "    #              'idf1': 'IDF1', 'idp': 'IDP', 'idr': 'IDR', 'recall': 'Rcll', \\\n",
    "    #             'precision': 'Prcn',  \\\n",
    "    #             'mostly_tracked' : 'MT', 'partially_tracked': 'PT', \\\n",
    "    #             'mostly_lost' : 'ML', 'num_false_positives': 'FP', \\\n",
    "    #             'num_misses': 'FN', 'num_switches' : 'IDsw', \\\n",
    "    #             'num_fragmentations' : 'FM'\n",
    "    #             }\n",
    "    # )\n",
    "\n",
    "    df =  pd.DataFrame.from_dict(summary)\n",
    "    namemap = {'num_objects': 'GT', 'mota': 'MOTA', 'motp' : 'MOTP', \\\n",
    "                 'idf1': 'IDF1', 'idp': 'IDP', 'idr': 'IDR', 'recall': 'Rcll', 'precision': 'Prcn',  \\\n",
    "                'mostly_tracked' : 'MT', 'partially_tracked': 'PT', \\\n",
    "                'mostly_lost' : 'ML', 'num_false_positives': 'FP', \\\n",
    "                'num_misses': 'FN', 'num_switches' : 'IDsw', \\\n",
    "                'num_fragmentations' : 'FM', 'num_frames': 'Frames'\n",
    "                }\n",
    "    df.rename(columns=namemap, inplace=True)\n",
    "    return df\n",
    "\n",
    "# tkl = []\n",
    "# pt = '../tracking_results'\n",
    "# for trk in os.listdir(pt):\n",
    "#     for param in os.listdir(os.path.join(pt, trk)):\n",
    "#         tkl.append([trk, param, trackerlist(trk, param, None)[0]])\n",
    "# pprint(tkl,compact=True,width=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(tkl)):\n",
    "#     result = one_track(tkl[i][-1])\n",
    "#     trk,param = tkl[i][0], tkl[i][1]\n",
    "#     result.to_csv(os.path.join('../tracking_csv', 'result_{}_{}.csv'.format(trk, param)), index=False)\n",
    "\n",
    "def sam_track():\n",
    "    names = []\n",
    "    accs = []\n",
    "\n",
    "    for seq_id, seq in enumerate(dataset):\n",
    "        gt = []\n",
    "        pred = []\n",
    "        for (obj_id, obj_gt) in list(seq.ground_truth_rect.items()):\n",
    "            for frame_id, b in enumerate(obj_gt):\n",
    "                if b[0] == -1: continue\n",
    "                gt.append([frame_id+1,255-int(obj_id),b[0],b[1],b[2],b[3],1.,-1.,-1.,-1.])\n",
    "        pred_base_results = '/mnt/data/qizhezhang/TSAMs/segment-anything-2/results'\n",
    "        with open(os.path.join(pred_base_results, seq.name+'.txt')) as f:\n",
    "            lines = f.readlines() \n",
    "            for i in range(1,len(lines)):\n",
    "                pred.append([int(i) for i in lines[i].split(',')])\n",
    "                \n",
    "        # 把pred和gt分别按照第一个数、第二个排序\n",
    "        pred = sorted(pred, key=lambda x: (x[0], x[1]))\n",
    "        gt = sorted(gt, key=lambda x: (x[0], x[1]))\n",
    "        \n",
    "        accs.append(motMetricsEnhancedCalculator(np.array(gt, dtype=np.float64), np.array(pred, dtype=np.float64)))\n",
    "        names.append(seq.name)\n",
    "\n",
    "        # # 把gt和pred写到两个文件里\n",
    "        # with open('gt.txt', 'w') as f:\n",
    "        #     for i in gt:\n",
    "        #         f.write(','.join([str(j) for j in i])+'\\n')\n",
    "        # with open('pred.txt', 'w') as f:\n",
    "        #     for i in pred:\n",
    "        #         f.write(','.join([str(j) for j in i])+'\\n')\n",
    "                \n",
    "        # break\n",
    "\n",
    "    mh = mm.metrics.create()\n",
    "    summary = mh.compute_many(accs, metrics=['mota', 'motp', 'num_frames', 'num_objects' , 'idf1', 'idp', 'idr', \\\n",
    "                                        'recall', 'precision',  \\\n",
    "                                        'mostly_tracked', 'partially_tracked', \\\n",
    "                                        'mostly_lost', 'num_false_positives', \\\n",
    "                                        'num_misses', 'num_switches', \\\n",
    "                                        'num_fragmentations'\n",
    "                                    ], \\\n",
    "                        generate_overall=True,\n",
    "                        names=names)\n",
    "\n",
    "    df =  pd.DataFrame.from_dict(summary)\n",
    "    namemap = {'num_objects': 'GT', 'mota': 'MOTA', 'motp' : 'MOTP', \\\n",
    "                 'idf1': 'IDF1', 'idp': 'IDP', 'idr': 'IDR', 'recall': 'Rcll', 'precision': 'Prcn',  \\\n",
    "                'mostly_tracked' : 'MT', 'partially_tracked': 'PT', \\\n",
    "                'mostly_lost' : 'ML', 'num_false_positives': 'FP', \\\n",
    "                'num_misses': 'FN', 'num_switches' : 'IDsw', \\\n",
    "                'num_fragmentations' : 'FM', 'num_frames': 'Frames'\n",
    "                }\n",
    "    df.rename(columns=namemap, inplace=True)\n",
    "    return df\n",
    "\n",
    "result = sam_track()\n",
    "result.to_csv(os.path.join('../tracking_csv', 'result_sam.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sam_visualize():\n",
    "    from collections import defaultdict\n",
    "    seq = dataset[0]\n",
    "    gt = defaultdict(dict)\n",
    "    pred = defaultdict(dict)\n",
    "    \n",
    "    for (obj_id, obj_gt) in list(seq.ground_truth_rect.items()):\n",
    "        for frame_id, b in enumerate(obj_gt):\n",
    "            gt[frame_id][obj_id] = [int(i) for i in b]\n",
    "    pred_base_results = '/mnt/data/qizhezhang/TSAMs/segment-anything-2/results'\n",
    "    with open(os.path.join(pred_base_results, seq.name+'.txt')) as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.split(',') # lines读到的信息是[frame_id+1,obj_id,b[0],b[1],b[2],b[3],1.,-1.,-1.,-1.]\n",
    "            pred[int(l[0])-1][int(l[1])] = [int(i) for i in l[2:6]]\n",
    "    \n",
    "    # 把gt和pred分别写到两个文件\n",
    "    \n",
    "    with open('sam_visualize/gt.txt', 'w') as f:\n",
    "        for i in range(len(gt)):\n",
    "            for obj_id, b in gt[i].items():\n",
    "                f.write(f'{i+1},{obj_id},{b[0]},{b[1]},{b[2]},{b[3]}\\n')\n",
    "                \n",
    "    with open('sam_visualize/pred.txt', 'w') as f:\n",
    "        for i in range(len(pred)):\n",
    "            for obj_id, b in pred[i].items():\n",
    "                f.write(f'{i+1},{obj_id},{b[0]},{b[1]},{b[2]},{b[3]}\\n')\n",
    "    \n",
    "    \n",
    "    def draw(i):\n",
    "        img_path = seq.frames[i]\n",
    "        import cv2\n",
    "        img1 = cv2.imread(img_path)\n",
    "        img2 = cv2.imread(img_path)\n",
    "        \n",
    "        for obj_id, b in gt[i].items():\n",
    "            cv2.rectangle(img1, (b[0],b[1]), (b[0]+b[2],b[1]+b[3]), (0,255,0), 1)\n",
    "            cv2.putText(img1, str(255-int(obj_id)), (b[0],b[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 182, 193), 1)\n",
    "        for obj_id, b in pred[i].items():\n",
    "            cv2.rectangle(img2, (b[0],b[1]), (b[0]+b[2],b[1]+b[3]), (0,0,255), 1)\n",
    "            cv2.putText(img2, str(obj_id), (b[0],b[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,200,200), 1)\n",
    "            \n",
    "        # 把两张图片并排显示\n",
    "        img = np.hstack((img1, img2))\n",
    "        # save\n",
    "        cv2.imwrite(f'sam_visualize/visualize_{i}.png', img)\n",
    "        \n",
    "    # for i in range(len(gt)):\n",
    "    #     draw(i)\n",
    "            \n",
    "    \n",
    "sam_visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
